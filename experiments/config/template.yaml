mode: train # (train / eval)
logger:
  level: 20 # CRITICAL = 50, ERROR = 40, WARNING = 30, INFO = 20, DEBUG = 10, NOTSET = 0
  format: "%(asctime)s - %(levelname)s - %(module)s - %(message)s"
experiment_path: .\results

gym_environment:
  shares: 2
  money: 100000
  data:
    path: .\data\example
    symbol: AAPL
    attributes: [ "time", "open", "close", "low", "high" ]
    n_peers: -1 # -1 all, 0 none, n > 0 else TODO implement
    social_lookback: 1 # n > 0 in days
    start: "2022-08-18 01:00:00"
    end: "2022-08-18 23:59:59"

agent:
  policy: MlpPolicy
  device: cpu # (cuda / cpu / auto)
  verbose: 0 # 0 none, 1 training information, 2 debug
  save_path: "./result_dqn_trading"
  epochs: 100
  log_interval: 5

  # define model with its specific parameters
  model:
    name: DQN
    pretrained_path: # empty if start from scratch TODO implement
    specific_parameters:
      learning_rate: 0.0005
      buffer_size: 50000
      learning_starts: 50000
      batch_size: 32
      tau: 1.0
      gamma: 0.99
      train_freq: 4
      gradient_steps: 1
      # add more configurable model-depending parameters here
