mode: "train" # (train / eval)
logger:
  level: 20 # CRITICAL = 50, ERROR = 40, WARNING = 30, INFO = 20, DEBUG = 10, NOTSET = 0
  format: "%(asctime)s - %(levelname)s - %(module)s - %(message)s"
experiment_path: ".\results"

gym_environment:
  shares: 2
  money: 100000
  data:
    path: ".\data"
    symbols: ["AAPL"]
    attributes: ["time", "open", "close", "low", "high"]

agent:
  policy: "MlpPolicy"
  device: "cpu" # (cuda / cpu / auto)
  verbose: 0 # 0 none, 1 training information, 2 debug
  save_path: "./result_dqn_trading"

  # define model with its specific parameters
  model:
    name: "DQN"
    pretrained_path: # empty if start from scratch
    learning_rate: 0.0005 # TODO
    buffer_size: 50000 # TODO
    learning_starts: 50000 # TODO
    batch_size: 32 # TODO
    tau: 1.0 # TODO
    gamma: 0.99 # TODO
    train_freq: 4 # TODO
    gradient_steps: 1 # TODO
    # add more configurable model-depending parameters here
