mode: train # (train / eval)
logger:
  level: 20 # CRITICAL = 50, ERROR = 40, WARNING = 30, INFO = 20, DEBUG = 10, NOTSET = 0
  format: "%(asctime)s - %(levelname)s - %(module)s - %(message)s"
experiment_path: ./experiments/results/ohlc_tick_twitter/a2c
test_policy: true # whether to test / apply the policy after training or not

# define gym environment that will be injected to the agent
gym_environment:
  enable_render: false # whether to render the policy application or not
  window_size: 30
  scale_reward: 1
  data:
    train_path: ./experiments/data/minmax/train.csv
    test_path: ./experiments/data/minmax/test.csv
    attributes: ["open", "high", "low", "close", "tick_volume", "spread", "real_volume", "bid_count", "bid_mean", "bid_std", "bid_min", "bid_25%", "bid_50%", "bid_75%", "bid_max", "ask_count", "ask_mean", "ask_std", "ask_min", "ask_25%", "ask_50%", "ask_75%", "ask_max", "last_count", "last_mean", "last_std", "last_min", "last_25%", "last_50%", "last_75%", "last_max", "volume_count", "volume_mean", "volume_std", "volume_min", "volume_25%", "volume_50%", "volume_75%", "volume_max", "time_msc_count", "time_msc_mean", "time_msc_std", "time_msc_min", "time_msc_25%", "time_msc_50%", "time_msc_75%", "time_msc_max", "flags_count", "flags_mean", "flags_std", "flags_min", "flags_25%", "flags_50%", "flags_75%", "flags_max", "volume_real_count", "volume_real_mean", "volume_real_std", "volume_real_min", "volume_real_25%", "volume_real_50%", "volume_real_75%", "volume_real_max", "twitter_mention_count", "twitter_mention_mean", "twitter_mention_std", "twitter_mention_min", "twitter_mention_25%", "twitter_mention_50%", "twitter_mention_75%", "twitter_mention_max", "twitter_postitive_score_count", "twitter_postitive_score_mean", "twitter_postitive_score_std", "twitter_postitive_score_min", "twitter_postitive_score_25%", "twitter_postitive_score_50%", "twitter_postitive_score_75%", "twitter_postitive_score_max", "twitter_negative_score_count", "twitter_negative_score_mean", "twitter_negative_score_std", "twitter_negative_score_min", "twitter_negative_score_25%", "twitter_negative_score_50%", "twitter_negative_score_75%", "twitter_negative_score_max", "twitter_positive_mention_count", "twitter_positive_mention_mean", "twitter_positive_mention_std", "twitter_positive_mention_min", "twitter_positive_mention_25%", "twitter_positive_mention_50%", "twitter_positive_mention_75%", "twitter_positive_mention_max", "twitter_negative_mention_count", "twitter_negative_mention_mean", "twitter_negative_mention_std", "twitter_negative_mention_min", "twitter_negative_mention_25%", "twitter_negative_mention_50%", "twitter_negative_mention_75%", "twitter_negative_mention_max", "twitter_score_count", "twitter_score_mean", "twitter_score_std", "twitter_score_min", "twitter_score_25%", "twitter_score_50%", "twitter_score_75%", "twitter_score_max"]

# define the rl agent (using the above gym environment)
agent:
  episodes: 100
  log_interval: 5
  sb_logger: ["stdout", "csv"] # format options are: "stdout", "csv", "log", "tensorboard", "json"

  # define model with its specific parameters
  model:
    name: A2C
    pretrained_path: # empty if start from scratch TODO implement
    policy: MlpPolicy
    device: auto # (cuda / cpu / auto)
    verbose: 0 # 0 none, 1 training information, 2 debug
    learning_rate: 0.0007
    gamma: 0.99
    seed: 1
    n_steps: 2048
    gae_lambda: 0.95
    ent_coef: 0.0
    vf_coef: 0.5
    rms_prop_eps: 0.00005
    use_rms_prop: true
    normalize_advantage: true