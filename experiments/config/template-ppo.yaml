mode: train # (train / eval)
logger:
  level: 20 # CRITICAL = 50, ERROR = 40, WARNING = 30, INFO = 20, DEBUG = 10, NOTSET = 0
  format: "%(asctime)s - %(levelname)s - %(module)s - %(message)s"
experiment_path: .\results

# define gym environment that will be injected to the agent
gym_environment:
  enable_render: true
  window_size: 6
  data:
    path: .\data\example
    symbol: AAPL
    attributes: [ "time", "open", "close", "low", "high" ]
    n_peers: -1 # -1 all, 0 none, n > 0 else TODO implement
    social_lookback: 1 # n > 0 in days
    training_start: "2022-01-18 01:00:00"
    training_end: "2022-06-18 23:59:59"
    testing_start: "2022-06-18 01:00:00"
    testing_end: "2022-08-18 23:59:59"

# define the rl agent (using the above gym environment)
agent:
  epochs: 10000
  log_interval: 5
  sb_logger: ["stdout", "csv", "tensorboard"] # format options are: "stdout", "csv", "log", "tensorboard", "json"

  # define model with its specific parameters
  model:
    name: PPO
    pretrained_path: # empty if start from scratch TODO implement
    policy: MlpPolicy
    device: cpu # (cuda / cpu / auto)
    verbose: 0 # 0 none, 1 training information, 2 debug
    learning_rate: 0.0005
    gamma: 0.99
    seed: null
    n_steps: 2048
    batch_size: 32
    n_epochs: 10
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    normalize_advantage: true
    ent_coef: 0.0
    vf_coef: 0.5
    target_kl: null